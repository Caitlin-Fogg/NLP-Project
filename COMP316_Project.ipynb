{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caitlin-Fogg/NLP-Project/blob/main/COMP316_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1n6epDUpTuq"
      },
      "outputs": [],
      "source": [
        "#Group members:\n",
        "#Caitlin Fogg 223005053\n",
        "#Tarika Sukdeoa 223010024\n",
        "#Naseeha Osman 223005931"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3c215f6-1541-4e11-b929-6265735f9c08",
        "id": "hgDH1DgzFuxw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              },
              "id": "3eb57b3ac8f44a14a94b0bdc81a8b92f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.0.12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total resumes: 9544\n",
            "Training resumes: 7635\n",
            "Testing resumes: 1909\n",
            "Preprocessing complete\n",
            "Word embedding model complete\n",
            "Start testing\n",
            "Processed 0 resumes\n",
            "Processed 100 resumes\n",
            "Processed 200 resumes\n",
            "Processed 300 resumes\n",
            "Processed 400 resumes\n",
            "Processed 500 resumes\n",
            "Processed 600 resumes\n",
            "Processed 700 resumes\n",
            "Processed 800 resumes\n",
            "Processed 900 resumes\n",
            "Processed 1000 resumes\n",
            "Processed 1100 resumes\n",
            "Processed 1200 resumes\n",
            "Processed 1300 resumes\n",
            "Processed 1400 resumes\n",
            "Processed 1500 resumes\n",
            "Processed 1600 resumes\n",
            "Processed 1700 resumes\n",
            "Processed 1800 resumes\n",
            "Processed 1900 resumes\n",
            "Testing complete.\n",
            "Evaluation on Test Data:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "          skills       0.35      0.96      0.52     67022\n",
            "   cert_provider       0.00      0.00      0.00      1054\n",
            "         company       0.78      0.36      0.50     11367\n",
            "          degree       0.67      0.08      0.14      5710\n",
            "   activity_type       1.00      0.01      0.02      1520\n",
            "           major       0.93      0.18      0.30      5796\n",
            " cert_issue_date       0.00      0.00      0.00       478\n",
            "      grade_type       0.63      0.10      0.17      2219\n",
            "        location       1.00      0.43      0.60     10117\n",
            "cert_expiry_date       0.00      0.00      0.00        91\n",
            "      start_date       0.57      0.29      0.39      8342\n",
            "      job_skills       0.72      0.38      0.50     29886\n",
            "  language_level       0.00      0.00      0.00       409\n",
            "     institution       0.59      0.17      0.26      9685\n",
            "       languages       0.00      0.00      0.00       315\n",
            "           grade       0.56      0.16      0.24      2698\n",
            "        end_date       0.90      0.09      0.16      7662\n",
            "  responsibility       0.85      0.25      0.38     46230\n",
            "       grad_year       0.80      0.09      0.17      3158\n",
            "       objective       0.82      0.55      0.66     20647\n",
            "     cert_skills       0.00      0.00      0.00       627\n",
            "       job_title       0.97      0.25      0.40     13689\n",
            "    activity_org       0.53      0.12      0.19      2274\n",
            "\n",
            "        accuracy                           0.47    250996\n",
            "       macro avg       0.55      0.19      0.24    250996\n",
            "    weighted avg       0.67      0.47      0.44    250996\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Main solution - HMM model\n",
        "\n",
        "# Imports\n",
        "!pip uninstall gensim -y\n",
        "!pip install --upgrade gensim\n",
        "!pip install cython\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/resume_data.csv\")\n",
        "\n",
        "# Renaming columns to more common terms\n",
        "column_map = {\n",
        "    'career_objective': 'objective',\n",
        "    'skills': 'skills',\n",
        "    'educational_institution_name': 'institution',\n",
        "    'degree_names': 'degree',\n",
        "    'passing_years': 'grad_year',\n",
        "    'educational_results': 'grade',\n",
        "    'result_types': 'grade_type',\n",
        "    'major_field_of_studies': 'major',\n",
        "    'professional_company_names': 'company',\n",
        "    'company_urls': 'company_url',\n",
        "    'start_dates': 'start_date',\n",
        "    'end_dates': 'end_date',\n",
        "    'related_skils_in_job': 'job_skills',\n",
        "    'positions': 'job_title',\n",
        "    'locations': 'location',\n",
        "    'responsibilities': 'responsibility',\n",
        "    'extra_curricular_activity_types': 'activity_type',\n",
        "    'extra_curricular_organization_names': 'activity_org',\n",
        "    'extra_curricular_organization_links': 'activity_link',\n",
        "    'role_positions': 'activity_role',\n",
        "    'languages': 'languages',\n",
        "    'proficiency_levels': 'language_level',\n",
        "    'certification_providers': 'cert_provider',\n",
        "    'certification_skills': 'cert_skills',\n",
        "    'online_links': 'cert_link',\n",
        "    'issue_dates': 'cert_issue_date',\n",
        "    'expiry_dates': 'cert_expiry_date',\n",
        "    '\\ufeffjob_position_name': 'job_position',\n",
        "    'educationaL_requirements': 'job_edu_req',\n",
        "    'experiencere_requirement': 'job_exp_req',\n",
        "    'age_requirement': 'job_age_req',\n",
        "    'responsibilities.1': 'job_responsibility',\n",
        "    'skills_required': 'job_skills_req',\n",
        "    'matched_score': 'matched_score'\n",
        "}\n",
        "\n",
        "df.rename(columns=column_map, inplace=True)\n",
        "col_to_use = ['objective','skills', 'institution','degree','grad_year','grade','grade_type','major','company','start_date','end_date','job_skills','job_title','location','responsibility','activity_type','activity_org','languages','language_level','cert_provider','cert_skills','cert_issue_date','cert_expiry_date']\n",
        "def get_word_tags(row):\n",
        "    word_tag_pairs = []\n",
        "    stop_words = set(stopwords.words('english'))  # Get the set of English stop words\n",
        "    for col in col_to_use:\n",
        "        value = row[col]\n",
        "        # Checks if a value is missing\n",
        "        if pd.isna(value):\n",
        "            continue\n",
        "        # Convert list-like strings to lists if needed\n",
        "        if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
        "            try:\n",
        "                items = eval(value) # Uses eval to try convert it to an actual Python list\n",
        "            except:\n",
        "                items = [value] # If it fails, treat it like a single item list\n",
        "        elif isinstance(value, str):\n",
        "            items = re.split(r'[,\\n;]', value)\n",
        "        else:\n",
        "            items = [value]\n",
        "\n",
        "        for item in items:\n",
        "            # Check if item is not None and is a string before applying strip()\n",
        "            if item is not None and isinstance(item, str):\n",
        "                item = item.strip()  # convert item to string before strip, removes leading/trialing whitespaces\n",
        "            # If item is a string, proceed to split\n",
        "            if isinstance(item, str) and item:\n",
        "                for word in item.split():\n",
        "                    if word.lower() not in stop_words:  # Check if the word is a stop word\n",
        "                        word_tag_pairs.append((word.lower(), col.lower()))\n",
        "            # If item is a list, iterate through and split each element\n",
        "            elif isinstance(item, list):\n",
        "                for sub_item in item:\n",
        "                    if isinstance(sub_item, str):\n",
        "                        for word in sub_item.split():\n",
        "                            if word.lower() not in stop_words:  # Check if the word is a stop word\n",
        "                                word_tag_pairs.append((word.lower(), col.lower()))\n",
        "    return word_tag_pairs\n",
        "\n",
        "# Create full dataset of word-tag pairs (with stop words removed)\n",
        "all_data = []\n",
        "for _, row in df.iterrows():\n",
        "    all_data.append(get_word_tags(row))\n",
        "\n",
        "# Train/Test Split\n",
        "\n",
        "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Total resumes: {len(all_data)}\")\n",
        "print(f\"Training resumes: {len(train_data)}\")\n",
        "print(f\"Testing resumes: {len(test_data)}\")\n",
        "\n",
        "print(\"Preprocessing complete\")\n",
        "\n",
        "# Word embedding\n",
        "# Extracting words from training data\n",
        "def get_words(row):\n",
        "    sentence = []\n",
        "    for word, tag in row:\n",
        "        sentence.append(word)\n",
        "    return sentence\n",
        "\n",
        "train_sentences = []\n",
        "for row in train_data:\n",
        "  train_sentences.append(get_words(row))\n",
        "\n",
        "# Train model\n",
        "word2vec_embedding_model = Word2Vec(\n",
        "    sentences=train_sentences,  # Training data\n",
        "    vector_size = 100, # Word vectors size -(increse=more nuance but risk of overfitting) (lower = faster but lose small differences)\n",
        "    window = 5, # How many words before and after it looks at (increase - better for thematic similarity?) (decrease - syntatic and short range?)\n",
        "    min_count = 2,  # Ignores words that only appear <min_count (smaller better for smaller data sets but risk noise)(higher more stable model)\n",
        "    workers = 4, # No. of CPU cores used to train model\n",
        "    sg = 1) # Uses skip gram but can use (SG- slower, better for small datasets and specialized datasets) (CBOW = 0) (cbow - faster, better for large corpuses that are more general )\n",
        "\n",
        "# Save model\n",
        "word2vec_embedding_model.save(\"word2vec_embedding_model.model\")\n",
        "\n",
        "print(\"Word embedding model complete\")\n",
        "# Function to get word vectors\n",
        "def get_word_vector(word):\n",
        "    \"\"\"Get embedding vector for a word, returns zero vector if word not found\"\"\"\n",
        "    if word in word2vec_embedding_model.wv:\n",
        "        return word2vec_embedding_model.wv[word]\n",
        "    else:\n",
        "        return np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "# Extracting unique tags (NER labels)\n",
        "all_tags = set() # Use a set to ensure no duplicates\n",
        "for row in train_data:\n",
        "    for word, tag in row:\n",
        "        all_tags.add(tag)\n",
        "all_tags = list(all_tags) # Converts set to a list to enable indexing\n",
        "# Creates indices for the tags in a dictionary\n",
        "tag_to_index = {tag: i for i, tag in enumerate(all_tags)}\n",
        "index_to_tag = {i: tag for tag, i in tag_to_index.items()}\n",
        "\n",
        "# Setting up constants\n",
        "EMBEDDING_DIM = 100  # Must match Word2Vec embedding size\n",
        "NUM_TAGS = len(all_tags)\n",
        "\n",
        "# Emission probabilities using Word2Vec - we use cosine similarity between a word vector and the average vector for each tag\n",
        "# This is instead of using P(word|tag) from count probabilities\n",
        "\n",
        "# Calculate average embedding for each tag\n",
        "tag_vectors = defaultdict(list) # Creates a dictionary where each key (a tag) maps to a list of word vectors\n",
        "# Loops through each resume in train_data\n",
        "for row in train_data:\n",
        "  # Each row is a list of word tag pairs\n",
        "    for word, tag in row:\n",
        "      # Uses get_word_vector function\n",
        "        vec = get_word_vector(word)\n",
        "        # If the vector exists, add it to the list of vectors assocated with that tag\n",
        "        if vec is not None:\n",
        "            tag_vectors[tag].append(vec)\n",
        "\n",
        "# Creates an empty dictionary to hold the average vector for each tag\n",
        "tag_avg_vectors = {}\n",
        "# For each tag and its list of vectors: compute mean vector across all word vectors for that tag, store average vector\n",
        "for tag, vecs in tag_vectors.items():\n",
        "    if vecs:\n",
        "        tag_avg_vectors[tag] = np.mean(vecs, axis=0)\n",
        "    else:\n",
        "      # If there were no vectors, assign a zero vector tp avoid breaking the model\n",
        "        tag_avg_vectors[tag] = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "# Emission probability using cosine similarity between word vector and average vector\n",
        "# Function takes in word that we want to estimate the probability of being emitted from a tag\n",
        "# Cosine similarity took too long when comparing single comparisons therefore defined a faster method\n",
        "def fast_cosine_similarity(vec1, vec2):\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return np.dot(vec1, vec2) / (norm1 * norm2)\n",
        "\n",
        "def emission_probability(word, tag):\n",
        "    word_vec = get_word_vector(word)\n",
        "    tag_vec = tag_avg_vectors[tag]\n",
        "    return fast_cosine_similarity(word_vec, tag_vec)\n",
        "\n",
        "# Transition Probabilities\n",
        "# Count transitions and initial states\n",
        "transition_counts = defaultdict(Counter) # A nested dictionary where transition_counts[prev_tag][curr_tag] gives the count of how often curr_tag follows prev_tag\n",
        "initial_tag_counts = Counter() # A counter for how many times each tag is the first tag in a resume\n",
        "\n",
        "# Loops through training data, skipping empty rows\n",
        "for row in train_data:\n",
        "    if not row:\n",
        "        continue\n",
        "    initial_tag_counts[row[0][1]] += 1 # For each resume, look at the first tag (row[0][1]) and increment its count\n",
        "    # For every pair of consecutive tags in a resume, count how often curr_tag follows prev_tag\n",
        "    for i in range(1, len(row)):\n",
        "        prev_tag = row[i - 1][1]\n",
        "        curr_tag = row[i][1]\n",
        "        transition_counts[prev_tag][curr_tag] += 1\n",
        "\n",
        "# Normalise to probabilities\n",
        "# These dictionaries will hold the final probabilities\n",
        "transition_probs = {}\n",
        "initial_probs = {}\n",
        "\n",
        "V = len(all_tags)  # Total number of unique tags (for smoothing)\n",
        "\n",
        "# Compute inital tag probabilities\n",
        "total_initial = sum(initial_tag_counts.values()) + V  # Add V for Laplace smoothing\n",
        "for tag in all_tags:\n",
        "    initial_probs[tag] = (initial_tag_counts[tag] + 1) / total_initial # + 1 for Laplace smoothing\n",
        "\n",
        "# Compute transition probabilities\n",
        "for prev_tag in all_tags:\n",
        "    total = sum(transition_counts[prev_tag].values()) + V # Counts total amount of times prev_tag occured, added V for smoothing\n",
        "    transition_probs[prev_tag] = {}\n",
        "    # For each possible curr_tag, compute the probability of it occurring after prev_tag\n",
        "    for curr_tag in all_tags:\n",
        "        if total == 0:\n",
        "            transition_probs[prev_tag][curr_tag] = 0\n",
        "        else:\n",
        "            transition_probs[prev_tag][curr_tag] = (transition_counts[prev_tag][curr_tag] + 1) / total\n",
        "\n",
        "# Viterbi algorithm\n",
        "# Given a sentence it returns the most likely sequence of tags\n",
        "def viterbi(sentence):\n",
        "    V = [{}]  # List of dictionaries - stores max probabilities for each tag in sentence\n",
        "    path = {}  # Backpointer - stores the best tag path leading to each tag\n",
        "\n",
        "    # Initialisation step (for the first word)\n",
        "    for tag in all_tags:\n",
        "      # Computes P(tag at position 0) = initial_probs[tag] × emission_prob(word_0 | tag)\n",
        "       # Uses .get() to return a small value (1e-6) if tag is not in initial_probs, for smoothing\n",
        "        V[0][tag] = initial_probs.get(tag, 1e-6) * emission_probability(sentence[0], tag)\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    # Recursion step - words 1 to n - finds the best path up to that tag\n",
        "    for t in range(1, len(sentence)):\n",
        "        V.append({}) # Create a new dictionary\n",
        "        new_path = {} # Temporary path to build updated best paths for each tag at a time\n",
        "        # Try each possible tag at the current word position\n",
        "        for curr_tag in all_tags:\n",
        "          # Find the best previous tag that leads to this current tag with the highest probability\n",
        "            (prob, prev_tag) = max(\n",
        "                (V[t - 1][prev_tag] * transition_probs[prev_tag].get(curr_tag, 1e-6) * emission_probability(sentence[t], curr_tag), prev_tag)\n",
        "                for prev_tag in all_tags\n",
        "            )\n",
        "            V[t][curr_tag] = prob # Store max probability\n",
        "            new_path[curr_tag] = path[prev_tag] + [curr_tag] # Append next best tag\n",
        "        path = new_path # Update path\n",
        "\n",
        "    # Termination step\n",
        "    # Find the tag at the last word (V[-1]) with the highest probablity. V[-1][tag] gives you the probability of the best path ending in tag\n",
        "    (prob, final_tag) = max((V[-1][tag], tag) for tag in all_tags)\n",
        "    return path[final_tag] # Return the full best tag path that ends in final_tag\n",
        "\n",
        "print(\"Start testing\")\n",
        "# Evaluation and testing\n",
        "# Flatten true and predicted tags\n",
        "true_tags = []\n",
        "pred_tags = []\n",
        "\n",
        "for i, resume in enumerate(test_data):\n",
        "    if not resume:\n",
        "        continue\n",
        "    words = [word for word, _ in resume]\n",
        "    true_tags_temp = [tag for _, tag in resume]\n",
        "    pred_tags_temp = viterbi(words)\n",
        "\n",
        "    true_tags.extend(true_tags_temp)\n",
        "    pred_tags.extend(pred_tags_temp)\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processed {i} resumes\") # Prints after every 100 resumes are processed to track progress\n",
        "\n",
        "print(\"Testing complete.\")\n",
        "# Generate evaluation metrics\n",
        "print(\"Evaluation on Test Data:\")\n",
        "print(classification_report(true_tags, pred_tags, labels=all_tags))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark model - BiLSTM-CRF model with our Word2Vec embeddings\n",
        "# Imports\n",
        "!pip uninstall gensim -y\n",
        "!pip install --upgrade gensim\n",
        "!pip install cython\n",
        "!pip install torchcrf\n",
        "\n",
        "!pip install git+https://github.com/kmkurn/pytorch-crf.git\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# Create dataloader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/resume_data.csv\")\n",
        "\n",
        "# Renaming columns to more common terms\n",
        "column_map = {\n",
        "    'career_objective': 'objective',\n",
        "    'skills': 'skills',\n",
        "    'educational_institution_name': 'institution',\n",
        "    'degree_names': 'degree',\n",
        "    'passing_years': 'grad_year',\n",
        "    'educational_results': 'grade',\n",
        "    'result_types': 'grade_type',\n",
        "    'major_field_of_studies': 'major',\n",
        "    'professional_company_names': 'company',\n",
        "    'company_urls': 'company_url',\n",
        "    'start_dates': 'start_date',\n",
        "    'end_dates': 'end_date',\n",
        "    'related_skils_in_job': 'job_skills',\n",
        "    'positions': 'job_title',\n",
        "    'locations': 'location',\n",
        "    'responsibilities': 'responsibility',\n",
        "    'extra_curricular_activity_types': 'activity_type',\n",
        "    'extra_curricular_organization_names': 'activity_org',\n",
        "    'extra_curricular_organization_links': 'activity_link',\n",
        "    'role_positions': 'activity_role',\n",
        "    'languages': 'languages',\n",
        "    'proficiency_levels': 'language_level',\n",
        "    'certification_providers': 'cert_provider',\n",
        "    'certification_skills': 'cert_skills',\n",
        "    'online_links': 'cert_link',\n",
        "    'issue_dates': 'cert_issue_date',\n",
        "    'expiry_dates': 'cert_expiry_date',\n",
        "    '\\ufeffjob_position_name': 'job_position',\n",
        "    'educationaL_requirements': 'job_edu_req',\n",
        "    'experiencere_requirement': 'job_exp_req',\n",
        "    'age_requirement': 'job_age_req',\n",
        "    'responsibilities.1': 'job_responsibility',\n",
        "    'skills_required': 'job_skills_req',\n",
        "    'matched_score': 'matched_score'\n",
        "}\n",
        "\n",
        "df.rename(columns=column_map, inplace=True)\n",
        "col_to_use = ['objective','skills', 'institution','degree','grad_year','grade','grade_type','major','company','start_date','end_date','job_skills','job_title','location','responsibility','activity_type','activity_org','languages','language_level','cert_provider','cert_skills','cert_issue_date','cert_expiry_date']\n",
        "def get_word_tags(row):\n",
        "    word_tag_pairs = []\n",
        "    stop_words = set(stopwords.words('english'))  # Get the set of English stop words\n",
        "    for col in col_to_use:\n",
        "        value = row[col]\n",
        "        # Checks if a value is missing\n",
        "        if pd.isna(value):\n",
        "            continue\n",
        "        # Convert list-like strings to lists if needed\n",
        "        if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
        "            try:\n",
        "                items = eval(value) # Uses eval to try convert it to an actual Python list\n",
        "            except:\n",
        "                items = [value] # If it fails, treat it like a single item list\n",
        "        elif isinstance(value, str):\n",
        "            items = re.split(r'[,\\n;]', value)\n",
        "        else:\n",
        "            items = [value]\n",
        "\n",
        "        for item in items:\n",
        "            # Check if item is not None and is a string before applying strip()\n",
        "            if item is not None and isinstance(item, str):\n",
        "                item = item.strip()  # convert item to string before strip, removes leading/trialing whitespaces\n",
        "            # If item is a string, proceed to split\n",
        "            if isinstance(item, str) and item:\n",
        "                for word in item.split():\n",
        "                    if word.lower() not in stop_words:  # Check if the word is a stop word\n",
        "                        word_tag_pairs.append((word.lower(), col.lower()))\n",
        "            # If item is a list, iterate through and split each element\n",
        "            elif isinstance(item, list):\n",
        "                for sub_item in item:\n",
        "                    if isinstance(sub_item, str):\n",
        "                        for word in sub_item.split():\n",
        "                            if word.lower() not in stop_words:  # Check if the word is a stop word\n",
        "                                word_tag_pairs.append((word.lower(), col.lower()))\n",
        "    return word_tag_pairs\n",
        "\n",
        "# Create full dataset of word-tag pairs (with stop words removed)\n",
        "all_data = []\n",
        "for _, row in df.iterrows():\n",
        "    all_data.append(get_word_tags(row))\n",
        "\n",
        "# Train/Test Split\n",
        "\n",
        "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Total resumes: {len(all_data)}\")\n",
        "print(f\"Training resumes: {len(train_data)}\")\n",
        "print(f\"Testing resumes: {len(test_data)}\")\n",
        "\n",
        "# Build vocabulary and tag mappings\n",
        "word_to_ix = defaultdict(lambda: len(word_to_ix)) # Maps each unique word to a unique index\n",
        "tag_to_ix = defaultdict(lambda: len(tag_to_ix)) # Maps each unique tag to a unique index\n",
        "\n",
        "# Add special padding tokens to both vocabs - used to pad shorter sequences\n",
        "word_to_ix[\"<PAD>\"]  # index 0 - placed before any words or tags\n",
        "tag_to_ix[\"<PAD>\"]\n",
        "\n",
        "# Populate mappings from the training data\n",
        "for sentence in train_data:\n",
        "    for word, tag in sentence:\n",
        "        word_to_ix[word.lower()]\n",
        "        tag_to_ix[tag]\n",
        "\n",
        "# Lock vocab - converts to regular dict to prevent accidental addition of new items during model training or inference\n",
        "word_to_ix = dict(word_to_ix)\n",
        "tag_to_ix = dict(tag_to_ix)\n",
        "# Create reverse mapping for tags\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "# Creates a list of all known words - helpful for debugging\n",
        "vocabulary = list(word_to_ix.keys())\n",
        "\n",
        "# Converting to indexed sequences - takes words/tags and returns a list of indices\n",
        "def encode_sentence(sentence, word_to_ix):\n",
        "    return [word_to_ix[word.lower()] for word, tag in sentence]\n",
        "\n",
        "def encode_tags(sentence, tag_to_ix):\n",
        "    return [tag_to_ix[tag] for word, tag in sentence]\n",
        "\n",
        "# Apply encoding to whole training set\n",
        "encoded_sentences = [encode_sentence(s, word_to_ix) for s in train_data]\n",
        "encoded_tags = [encode_tags(s, tag_to_ix) for s in train_data]\n",
        "\n",
        "# Convert to tensors - a generalisation of vectors to higher dimensions, allows for efficient numerical computations\n",
        "# Required for embedding layers and loss functions\n",
        "# X and Y are lists of 1D tensors\n",
        "X = [torch.tensor(seq, dtype=torch.long) for seq in encoded_sentences]\n",
        "y = [torch.tensor(seq, dtype=torch.long) for seq in encoded_tags]\n",
        "\n",
        "# Pad sequences to the same length for efficient batch processing\n",
        "# batch_first=True - means the resulting shape is batch_size, max_seq_len, padding_value specifies which index is used to fill the shorter sequences\n",
        "X_padded = pad_sequence(X, batch_first=True, padding_value=word_to_ix[\"<PAD>\"])\n",
        "y_padded = pad_sequence(y, batch_first=True, padding_value=tag_to_ix[\"<PAD>\"])\n",
        "# Create mask - used during model training to avoid computing loss on padded positions-\n",
        "mask = X_padded != word_to_ix[\"<PAD>\"]\n",
        "\n",
        "# Combines X_padded, y_padded and mask into a single TensorDataset\n",
        "dataset = TensorDataset(X_padded, y_padded, mask)\n",
        "# Dataloader provides batches of data to the model during training\n",
        "# Shuffle the data at every epoch to improve generalisation, also uses 2 background subprocesses for data loading (helps speed up training)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load the pretrained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"word2vec_embedding_model.model\")\n",
        "\n",
        "# Access the word vectors\n",
        "word_vectors = word2vec_model.wv\n",
        "\n",
        "# Initialise an embedding matrix with random values (size of vocab x embedding size)\n",
        "embedding_dim = 100  # Match the size used in Word2Vec model\n",
        "vocab_size = len(vocabulary)  # Size of vocabulary\n",
        "# Initialises a 2D numPy array (the embedding matrix) with random values\n",
        "# Holds either real Word2Vec vectors or random vectors if word is not in pretrained model\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim)\n",
        "\n",
        "# Creates a dictionary mapping each word in vocabulary to its corresponding row index in the embedding matrix\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "# Fill the embedding matrix with Word2Vec embeddings\n",
        "for word, idx in word_to_index.items():\n",
        "    if word in word_vectors:\n",
        "        embedding_matrix[idx] = word_vectors[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.randn(embedding_dim)  # for unknown words\n",
        "\n",
        "# Defining a neural network that performs sequence labeling (e.g. tagging words in resumes with labels like skill)\n",
        "# Inherits from nn.Module\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "  # Initialisation method\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, pretrained_embeddings=None):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        # Initialise embedding layer with pretrained Word2Vec embeddings\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings, dtype=torch.float))\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Hidden LSTM layer - outputs contextual representations\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        # Linear layer - Maps LSTM output to scores for each possible tag (emission scores for tokens)\n",
        "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
        "        # CRF layer - takes emission scores and models dependencies between tags i.e. which tag likely follows another tag\n",
        "        self.crf = CRF(tagset_size, batch_first=True)\n",
        "\n",
        "    # Forward pass - outputs emission scores fro every token in the sentence\n",
        "    def forward(self, sentence, mask=None):\n",
        "        embeds = self.embedding(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        emissions = self.fc(lstm_out)\n",
        "        return emissions\n",
        "\n",
        "    # Loss function - gets emission scores, applies the mask and Computes the negative log-likelihood loss using the CRF\n",
        "    # Returns a negative value in order to minimise loss\n",
        "    def loss(self, sentence, tags, mask):\n",
        "        emissions = self.forward(sentence)\n",
        "        # Apply mask to avoid padding positions\n",
        "        masked_emissions = emissions * mask.unsqueeze(-1)\n",
        "        return -self.crf(masked_emissions, tags, mask=mask)\n",
        "\n",
        "    # Prediction method - gets emission scores and Uses CRF decoding (Viterbi algorithm) to get most likely tag sequence\n",
        "    def predict(self, sentence, mask=None):\n",
        "        emissions = self.forward(sentence)\n",
        "        return self.crf.decode(emissions, mask=mask)\n",
        "\n",
        "# Initialise the model with the pretrained embeddings\n",
        "model = BiLSTM_CRF(vocab_size=len(vocabulary),\n",
        "                   embedding_dim=embedding_dim,\n",
        "                   hidden_dim=128,\n",
        "                   tagset_size=len(tag_to_ix),\n",
        "                   pretrained_embeddings=embedding_matrix)\n",
        "\n",
        "# model.parameters - This gives the optimizer access to all learnable parameters in the model so it can update them during training\n",
        "# lr - the learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    model.train() # Put model into training mode\n",
        "    total_loss = 0 # Used to monitor training process\n",
        "    for batch in loader:\n",
        "        x_batch, y_batch, mask_batch = batch\n",
        "\n",
        "        # Forward pass\n",
        "        loss = model.loss(x_batch, y_batch, mask=mask_batch)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()  # Zero gradients before backward pass to avoid gradient accumulation across batches\n",
        "        loss.backward()        # Backpropagate the loss - computes gradients of the loss with respect to each model parameter\n",
        "        optimizer.step()       # Update the model's weights\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Inference and evaluation\n",
        "# Take the first 100 samples from test_data\n",
        "subset_test_data = test_data[:100]\n",
        "\n",
        "# Encode sentences and tags\n",
        "subset_encoded_sentences = [encode_sentence(s, word_to_ix) for s in subset_test_data]\n",
        "subset_encoded_tags = [encode_tags(s, tag_to_ix) for s in subset_test_data]\n",
        "\n",
        "# Pad sequences and create mask\n",
        "X_subset = [torch.tensor(seq, dtype=torch.long) for seq in subset_encoded_sentences]\n",
        "y_subset = [torch.tensor(seq, dtype=torch.long) for seq in subset_encoded_tags]\n",
        "X_subset_padded = pad_sequence(X_subset, batch_first=True, padding_value=word_to_ix[\"<PAD>\"])\n",
        "y_subset_padded = pad_sequence(y_subset, batch_first=True, padding_value=tag_to_ix[\"<PAD>\"])\n",
        "mask_subset = X_subset_padded != word_to_ix[\"<PAD>\"]\n",
        "\n",
        "# Create DataLoader for the subset\n",
        "subset_dataset = TensorDataset(X_subset_padded, y_subset_padded, mask_subset)\n",
        "subset_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)  # No shuffling for evaluation\n",
        "\n",
        "model.eval() # Sets model to evaluation mode\n",
        "subset_preds = [] # Collects predicted labels\n",
        "subset_labels = [] # Collects true labels\n",
        "\n",
        "with torch.no_grad(): # Disables gradient calculation - saves memory and speeds up evaluation\n",
        "    for batch in subset_loader:\n",
        "        x_batch, y_batch, mask_batch = batch\n",
        "        # Use the model to predict the tag sequence for each sentence using CRF Viterbi decoding\n",
        "        predictions = model.predict(x_batch, mask=mask_batch)\n",
        "\n",
        "        # Process predictions and ground truth\n",
        "        for i in range(len(predictions)):\n",
        "            pred = predictions[i]\n",
        "            true = y_batch[i][mask_batch[i]].tolist()  # Get true tags where mask is True\n",
        "            subset_preds.extend(pred)\n",
        "            subset_labels.extend(true)\n",
        "\n",
        "# Convert tag indices back to tag names for evaluation\n",
        "pred_tag_names = [ix_to_tag[ix] for ix in subset_preds]\n",
        "true_tag_names = [ix_to_tag[ix] for ix in subset_labels]\n",
        "\n",
        "# Generate a classification report\n",
        "print(classification_report(true_tag_names, pred_tag_names, digits=4))\n"
      ],
      "metadata": {
        "id": "DNdURciKck-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d378189-900f-4519-f512-d3a9e2c99ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              },
              "id": "b174bd22697649f3a49ced244bf57e11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.0.12)\n",
            "Collecting torchcrf\n",
            "  Downloading TorchCRF-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchcrf) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchcrf) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->torchcrf)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->torchcrf) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->torchcrf) (3.0.2)\n",
            "Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m905.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchcrf\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchcrf-1.1.0\n",
            "Collecting git+https://github.com/kmkurn/pytorch-crf.git\n",
            "  Cloning https://github.com/kmkurn/pytorch-crf.git to /tmp/pip-req-build-2u34sl5z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/kmkurn/pytorch-crf.git /tmp/pip-req-build-2u34sl5z\n",
            "  Resolved https://github.com/kmkurn/pytorch-crf.git to commit 623e3402d00a2728e99d6e8486010d67c754267b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch-crf\n",
            "  Building wheel for pytorch-crf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-crf: filename=pytorch_crf-0.7.2-py3-none-any.whl size=6410 sha256=d50b35784169ac54981a6b8be08a68b35d9874b6458e6be434cccab8e3947a1a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ndrw40mw/wheels/fd/83/cc/f11543939f8911b8dcff86e2bd54423e21f779d0938958cc7f\n",
            "Successfully built pytorch-crf\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total resumes: 9544\n",
            "Training resumes: 7635\n",
            "Testing resumes: 1909\n",
            "Epoch 1 - Loss: 178161.5644\n",
            "Epoch 2 - Loss: 12170.4421\n",
            "Epoch 3 - Loss: 4036.8365\n",
            "Epoch 4 - Loss: 1779.1881\n",
            "Epoch 5 - Loss: 19014.4908\n",
            "Epoch 6 - Loss: 2622.9141\n",
            "Epoch 7 - Loss: 1159.7661\n",
            "Epoch 8 - Loss: 1437.9918\n",
            "Epoch 9 - Loss: 1602.9710\n",
            "Epoch 10 - Loss: 492.0304\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    activity_org     0.9854    1.0000    0.9926       135\n",
            "   activity_type     1.0000    0.9794    0.9896        97\n",
            "cert_expiry_date     1.0000    1.0000    1.0000         6\n",
            " cert_issue_date     0.9545    1.0000    0.9767        21\n",
            "   cert_provider     1.0000    0.9762    0.9880        42\n",
            "     cert_skills     1.0000    1.0000    1.0000        11\n",
            "         company     1.0000    1.0000    1.0000       620\n",
            "          degree     1.0000    1.0000    1.0000       336\n",
            "        end_date     1.0000    1.0000    1.0000       430\n",
            "       grad_year     1.0000    1.0000    1.0000       170\n",
            "           grade     1.0000    1.0000    1.0000       125\n",
            "      grade_type     1.0000    1.0000    1.0000       104\n",
            "     institution     1.0000    1.0000    1.0000       517\n",
            "      job_skills     1.0000    1.0000    1.0000      1577\n",
            "       job_title     1.0000    1.0000    1.0000       715\n",
            "  language_level     1.0000    1.0000    1.0000        18\n",
            "       languages     1.0000    1.0000    1.0000        15\n",
            "        location     1.0000    1.0000    1.0000       510\n",
            "           major     1.0000    1.0000    1.0000       339\n",
            "       objective     1.0000    1.0000    1.0000      1026\n",
            "  responsibility     1.0000    1.0000    1.0000      2327\n",
            "          skills     1.0000    1.0000    1.0000      3051\n",
            "      start_date     1.0000    1.0000    1.0000       467\n",
            "\n",
            "        accuracy                         0.9998     12659\n",
            "       macro avg     0.9974    0.9981    0.9977     12659\n",
            "    weighted avg     0.9998    0.9998    0.9998     12659\n",
            "\n"
          ]
        }
      ]
    }
  ]
}